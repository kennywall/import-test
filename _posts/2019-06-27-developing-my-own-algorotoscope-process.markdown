---
published: true
layout: post
title: Developing My Own Algorotoscope Process
date: 2019-06-24T09:00:00.000Z
tags:
  - Kin Lane
image: https://s3.amazonaws.com/kinlane-productions/algorotoscope-master/aws-s3-square-ellis-island-nazi-poster-square.jpg
---
[I first began doing Algorotoscope back in 2016 after the election](http://algorithmic.rotoscope.work/algorithmic-rotoscope/). I built on a process introduced by Algorithmia, but over time they stopped maintaining it, opting to offer a quicker, but lower quality solution in their marketplace. There was no real financial incentive for them to maintain the AWS AMI they had developed, and I couldn’t figure out what it needed to work, so I set out to develop my own. It took me over a year of fiddling around in my spare time before I figured out how to do it from start to finish, giving me back my older, higher quality, texture transfer machine learning process.

After some research I was able to develop the exact same process they had developed, but this time, I was able to shape the end result so that I can apply the filters as part of my own custom process. The first part of the process involves having a GPU server which I install Tensorflow, and a whole suite of other dependencies. Which gives me my texture transfer model training environment, where I can take any image I want, and train an ML model on the colors and textures present in the image. The process was all the craze for about 15 seconds a couple years back, and you saw startups popping up to turn any photo into a painting, but the interest quickly diminished, and everyone is onto the next incremental advancement in machine learning--hoping to strike it rich!

I am still captivated by the texture transfer, or as Algorithmia accurately called it, “Style Thief”. I like the way the ML training process meticulously “steals” the essence of a painting or a propaganda poster, and allows me to reapply to another image—algorithmically. I don’t steal the essence of a painting or poster because I think the results are “cool”, I do it because I can use the result to highlight how algorithms are impacting our world. The act of digitally re-appropriating someone else’s work with an algorithm reflects so much of the illness that technology injects into our world today, so I might as use the master's tool to shine an algorithmically generating light on other areas in which algorithms are impacting us.

Each machine learning model I train costs me upwards of $100.00, so I don’t do it carelessly. I pick specific pieces of art, propaganda posters, or other hand-crafted works. Ironically, training an ML model on another piece of digital work doesn’t result in there being my texture transferred, or style stolen. I’m looking to capture the essence of certain pieces of work, then developing a sort of lens for shining a narrative light on a particular subject such as the immigration debate, Russian election interference, and other critical issues we find ourselves up against these days. I’m not just looking to shine pretty colors on the discussion, I’m looking to distort other relevant, thought provoking images, while also highlight how algorithms are being used to twist and distort how we view, discuss, and understand critical issues.
<p><img src="https://s3.amazonaws.com/kinlane-productions/algorotoscope-master/aws-s3-stories-crypto-machine-bletchley-copper-circuit.png" align="right" width="45%" style="padding: 15px;"></p>
I already have my circuit board, and nazi propaganda poster models trained, recreating a couple staple filters I have applied to images in the past, highlighting Ellis Island as part of the immigration debate, and a host of technology introduces issues that plague us. I’m currently recreating another model I had developed in the past which was trained on a Russian propaganda poster, where they were distributing leaflets on the people. Next I’m doing Uncle Sam, Norman Rockwell’s Ruby Bridges, and a United Farm Workers poster. This should give me a basic color palette to work with when applying to different photos I’ve taken, allowing me to apply across my regular storytelling across my blogs and expanding the pool of relevant, yet random images I publish as part of [The US Narrative Today](http://the.us.narrative.today.kinlane.com/).

I cannot articulate how happy I am to have full control over my Algorotoscope process again. There was no other way to generate the high quality ML models I had produced before, and needed in my work. Plus, I’m happy to have full control over not just the training of models, but also the applying of them to my photos. While it is costly for me to develop the models, and isn’t cheap to apply the models across my photography, I now have more control over the process. Despite the costs, I’m hoping to develop a pretty robust, and meaningful catalog of filters I can apply. I’m not in the business of hiding the works I used to train my models, to the contrary, I’m all about highlighting them, and being repentant in my algorithmic thievery. In the end I’m just looking to bring attention to the ways In which algorithms are distorting our reality, hijacking the truth, and I find that storytelling, with a little splash of imagery is the most valuable tool I have in my toolbox.
